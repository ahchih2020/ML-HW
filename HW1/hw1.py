# -*- coding: utf-8 -*-
"""HW1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fM_WMJLn2d_oalw-mUnP1cXGrcE0PQ97

# **Homework 1: COVID-19 Cases Prediction (Regression)**
預測美國某一州Covid 19的人口比例，會給前5天數值，運用前4天值訓練，要預測第5天中Covid 19百分比

Dataset(Kaggle) https://www.kaggle.com/c/ml2022spring-hw1/data

Hints

simple : sample code

medium : Feature selection

strong : Different model architectures and optimizers

boss : L2 regularization and try more parameters

# Download Data
"""

tr_path = 'covid.train.csv'  # path to training data
tt_path = 'covid.test.csv'   # path to testing data

!gdown --id '1kJMuG_J5d98DruZXzgIid2jSAFiJ9I9B' --output covid.train.csv #從id處下載後，儲存成output後的檔案名字
!gdown --id '1a0tTYjv90WXxk02dOwlniw-6H0tPlyxd' --output covid.test.csv

"""# Import Packages"""

# Numerical Operations
import math
import numpy as np

# Reading/Writing Data
import pandas as pd
import os
import csv

# For Progress Bar
from tqdm import tqdm

# Pytorch
import torch 
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader, random_split

# For plotting learning curve
from torch.utils.tensorboard import SummaryWriter

"""# **Some Utilities Function**

You do not need to modify this part.
"""

def same_seed(seed): 
    '''Fixes random number generator seeds for reproducibility.'''
    #固定Random seed，幫助還原實驗結果 
    torch.backends.cudnn.deterministic = True #保證每次運行網絡的時候相同輸入的輸出是固定的
    torch.backends.cudnn.benchmark = False   #提升卷积神经网络的运行速度使用，但適合在卷積層設置一直改變時，反而會比較慢
    np.random.seed(seed)   #將numpy隨機亂數產生器的種子重新調整
    torch.manual_seed(seed)  #將pytorch隨機亂數產生器的種子重新調整
    if torch.cuda.is_available(): #如果CUDA被使用
        torch.cuda.manual_seed_all(seed) #重新為GPU上產生隨機數的種子進行設置

def train_valid_split(data_set, valid_ratio, seed): 
    '''Split provided training data into training set and validation set'''
    #從training data切出validation data(檢測model能力)
    valid_set_size = int(valid_ratio * len(data_set))  #先算valid_set的size
    train_set_size = len(data_set) - valid_set_size   #決定train_set的size
    train_set, valid_set = random_split(data_set, [train_set_size, valid_set_size], generator=torch.Generator().manual_seed(seed))
    #用random_split(dataset, lengths, generator=<torch._C.Generator object>)
    #來分割train/valid dataset
    return np.array(train_set), np.array(valid_set)

def predict(test_loader, model, device):
    #把預測結果寫成csv檔
    model.eval() # Set your model to evaluation mode.
           #不啟用Batch Normalization(用全部訓練數據的均值和方差，即測試過程中要保證BN層的均值和方差不變。
           #和Dropout(不進行隨機捨棄神經元)
    preds = []
    for x in tqdm(test_loader): #tqdm可擴展的Python進度條，可添加一個進度提示信息
        x = x.to(device)   #將所有最開始讀取數據時的tensor變量copy一份到device所指定的GPU上去，之後的運算都在GPU上進行。  
                    #後面衍生的變量自然也都在GPU上                   
        with torch.no_grad(): #with 語句適用於對資源進行訪問的場合，確保不管使用過程中是否發生異常都會執行必要的“clear”操作，釋放資源
                    #torch.no_grad()用來禁止梯度、反向傳導的計算，通常用來網絡推斷中，它可以減少計算內存的使用量，常和model.eval()搭配使用。
                    #不要忘記training在 eval 步驟後返回model.train()模式                  
            pred = model(x)                     
            preds.append(pred.detach().cpu())  #detach():返回一个new Tensor，阻断反向传播
                              #cpu():将維度放在cpu上，仍为tensor 
    preds = torch.cat(preds, dim=0).numpy()     #将两个张量（tensor）拼接在一起，按维数0（行）拼接A和B，也就是竖着拼接，A上B下
    return preds

"""# **Preprocess**

We have three kinds of datasets:
* `train`: for training
* `dev`: for validation
* `test`: for testing (w/o target value)

## **Dataset**

The `COVID19Dataset` below does:
* read `.csv` files
* extract features
* split `covid.train.csv` into train/dev sets
* normalize features

Finishing `TODO` below might make you pass medium baseline.
"""

class COVID19Dataset(Dataset):
    '''
    x: Features.
    y: Targets(data裡面的tested_positive欄位), if none, do prediction.
    '''
    def __init__(self, x, y=None):
        if y is None:
            self.y = y
        else:
            self.y = torch.FloatTensor(y) #建一個新的tensor(CPU類型,32bit浮點數)
        self.x = torch.FloatTensor(x)

    def __getitem__(self, idx):         #__getitem__(self,key)返回所給key對應的value
        if self.y is None:
            return self.x[idx]
        else:
            return self.x[idx], self.y[idx]

    def __len__(self):        #返回元素的個數
        return len(self.x)

"""# **Deep Neural Network Model**

`NeuralNet` is an `nn.Module` designed for regression.
The DNN consists of 2 fully-connected layers with ReLU activation.
This module also included a function `cal_loss` for calculating loss.
 
可修改的地方，嘗試不同model
"""

class My_Model(nn.Module):
    def __init__(self, input_dim):
        super(My_Model, self).__init__() #super可以调用父class
                         #在類的繼承中，如果重定義某個方法，該方法會覆蓋父類的同名方法，
                         #但有時，我們希望能同時實現父類的功能就可用super
        # TODO: modify model's structure, be aware of dimensions. 
        self.layers = nn.Sequential(  #神經網絡模塊將按照在傳入構造器的順序依次被添加到計算圖中執行
                         #裡面的模塊按照順序進行排列的，所以必須確保前一個模塊的輸出大小和下一個模塊的輸入大小是一致的
            nn.Linear(input_dim, 16), #用於線性轉換成y=x*A^T+b(將輸入變成不同維度,16)
                          #nn.Linear(input_tensor_size,output_tensor_size)
            nn.ReLU(),         #經過ReLU() Activation函數
            nn.Linear(16, 8),     #將輸入維度16轉換成8
            nn.ReLU(),
            nn.Linear(8, 1)      #將輸入維度8轉換成1
        )

    def forward(self, x):
        x = self.layers(x)        #Creating custom layers
        x = x.squeeze(1)         #把元素等於1的dimension去除
        return x

"""# Feature Selection
Choose features you deem useful by modifying the function below.
"""

def select_feat(train_data, valid_data, test_data, select_all=True):
    '''Selects useful features to perform regression'''
    #選出有用feature，select_all為True就是選所有特徵        #[:,:]表示[all rows,all columns]
    y_train, y_valid = train_data[:,-1], valid_data[:,-1]   #[:,-1]選取所有的row，但只選最後一欄
    raw_x_train, raw_x_valid, raw_x_test = train_data[:,:-1], valid_data[:,:-1], test_data #[:,:-1]選取所有row,column，除了最後一欄不選
                            #[::-1] 顺序相反操作，ex:a=[1,2,3]，a[::-1]=[3, 2, 1]；
                            #[:-1]從位置0到位置-1之前的数，a[:-1]=[1, 2]；
    if select_all:
        feat_idx = list(range(raw_x_train.shape[1])) #shape[0]輸出矩陣的row數量，shape[1]表示矩陣column數量
                                #range()產生從1開始的list
                                
    #上面select_all如果改為False，則可利用下面feat_idx選自己想要的feature
    else:
        feat_idx = [0,1,2,3,4] # TODO: Select suitable feature columns.
        
    return raw_x_train[:,feat_idx], raw_x_valid[:,feat_idx], raw_x_test[:,feat_idx], y_train, y_valid

np.savetxt("t1.csv",train_data,delimiter=",")

"""## **Training**"""

def trainer(train_loader, valid_loader, model, config, device):

    criterion = nn.MSELoss(reduction='mean') # Define your loss function, do not modify this.

    # Define your optimization algorithm. 
    # TODO: Please check https://pytorch.org/docs/stable/optim.html to get more available 優化演算法.
    # TODO: L2 regularization (optimizer(weight decay...) or implement by your self).

    #優化演算法，建一個優化器做SGD。lr為學習率，momentum用在計算參數更新方向前會考慮前一次參數更新的方向，通常設0.9
    optimizer = torch.optim.SGD(model.parameters(),lr=config['learning_rate'],momentum=0.9) ，
                                

    writer = SummaryWriter() #Writer of tensoboard.(好用的視覺化工具，可記錄數字，影像或者是聲音資訊)

    if not os.path.isdir('./models'):   #檢查目錄是否存在
        os.mkdir('./models')      #建一個models資料夾，存放參數用.

    n_epochs, best_loss, step, early_stop_count = config['n_epochs'], math.inf, 0, 0 # math.inf 表示正無窮大

    #每個data都會先做training(下面model.train())，再validation(下面model.eval)
    for epoch in range(n_epochs):
        #先做training
        model.train() # Set your model to train mode.
        loss_record=[]

        # tqdm is a package to visualize your training progress.
        train_pbar = tqdm(train_loader, position=0, leave=True)
        #在training，每個epoch，都會運用SGD來update參數，在training後進行validation，
        for x, y in train_pbar:
            optimizer.zero_grad()           # Set gradient to zero(將gradient所有參數的梯度緩衝區清空)
            x, y = x.to(device), y.to(device)   # Move your data to device. 
            pred = model(x)             
            loss = criterion(pred, y)    #用來計算loss
            loss.backward()          # Compute gradient(backpropagation).根據loss進行back propagation，計算gradient
            optimizer.step()          # Update parameters.(做gradient descent更新參數)
            step += 1
            loss_record.append(loss.detach().item())
            
            # Display current epoch number and loss on tqdm progress bar.
            train_pbar.set_description(f'Epoch [{epoch+1}/{n_epochs}]')
            train_pbar.set_postfix({'loss': loss.detach().item()})

        mean_train_loss = sum(loss_record)/len(loss_record)
        writer.add_scalar('Loss/train', mean_train_loss, step)

        #再做validation
        model.eval() # Set your model to evaluation mode(不是在training，而在預測(這邊是做validation)時，要調成evaluation mode)
        loss_record = []
        for x, y in valid_loader:
            x, y = x.to(device), y.to(device)
            with torch.no_grad():         #因為是進行預測(這邊是做validation)，不需要計算梯度，也不會進行反向傳播
                pred = model(x)        #也不更新參數，
                loss = criterion(pred, y)   #只進行validation loss運算

            loss_record.append(loss.item())
            
        mean_valid_loss = sum(loss_record)/len(loss_record)
        print(f'Epoch [{epoch+1}/{n_epochs}]:Train loss: {mean_train_loss:.4f}, Valid loss: {mean_valid_loss:.4f}')
        writer.add_scalar('Loss/valid', mean_valid_loss, step)  #將純量加到summary

        #early_stop:如果一直沒訓練結果讓他提早結束
        if mean_valid_loss < best_loss:
            best_loss = mean_valid_loss
            torch.save(model.state_dict(), config['save_path']) # Save your best model
            print('Saving model with loss {:.3f}...'.format(best_loss))
            early_stop_count = 0
        else: 
            early_stop_count += 1

        if early_stop_count >= config['early_stop']:
            print('\nModel is not improving, so we halt the training session.')
            return

"""# Configurations
`config` contains hyper-parameters for training and the path to save your model.
"""

device = 'cuda' if torch.cuda.is_available() else 'cpu'
#config是個字典檔，會再上面很多函數引用
config = {
    'seed': 5201314,      # Your seed number, you can pick your lucky number. :)
    'select_all': True,   # Whether to use all features.
    #預設是True(使用所有feature，但可調成false，選擇自己想要的feature)
    'valid_ratio': 0.2,   # validation_size = train_size * valid_ratio，切出20%比例做validation
    'n_epochs': 3000,     # Number of epochs.            
    'batch_size': 256, 
    'learning_rate': 1e-5,              
    'early_stop': 400,    # If model has not improved for this many consecutive epochs, stop training.     
    'save_path': './models/model.ckpt'  # Your model will be saved here.
}



"""# Dataloader
Read data from files and set up training, validation, and testing sets. You do not need to modify this part.
"""

# Set seed for reproducibility
same_seed(config['seed'])


# train_data size: 2699 x 118 (id + 37 states + 16 features x 5 days) 
# test_data size: 1078 x 117 (without last day's positive rate)
train_data, test_data = pd.read_csv('./covid.train.csv').values, pd.read_csv('./covid.test.csv').values #將資料導入
#用前面自定義函數train_valid_split從train.csv裡面打亂切出train、valid data
train_data, valid_data =train_valid_split(train_data, config['valid_ratio'], config['seed']) 

# Print out the data size.        #加f可在後面大括號內使用變數內容-train_data.shape值
print(f"""train_data size: {train_data.shape}  
valid_data size: {valid_data.shape} 
test_data size: {test_data.shape}""")

#用前面自定義函數select_feat()去Select features，x_train, x_valid, x_test為經過亂數選擇後，去掉最後一欄的資料
#y_train, y_valid為經過亂數選擇後，最後一欄的所有資料
x_train, x_valid, x_test, y_train, y_valid = select_feat(train_data, valid_data, test_data, config['select_all'])

# Print out the number of features.
print(f'number of features: {x_train.shape[1]}')

#呼叫前面的COVID19Dataset自定義函數
train_dataset, valid_dataset, test_dataset = COVID19Dataset(x_train,y_train),COVID19Dataset(x_valid,y_valid),COVID19Dataset(x_test)

# Pytorch data loader loads pytorch dataset into batches.
#在梯度下降的過程中，一般是需要將多個資料組成batch，需要DataLoader這個迭代器
train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True, pin_memory=True)
valid_loader = DataLoader(valid_dataset, batch_size=config['batch_size'], shuffle=True, pin_memory=True)
test_loader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=False, pin_memory=True)

"""# Start training!"""

model = My_Model(input_dim=x_train.shape[1]).to(device) # put your model and data on the same computation device.
trainer(train_loader, valid_loader, model, config, device)

"""# Plot learning curves with `tensorboard` (optional)

`tensorboard` is a tool that allows you to visualize your training progress.

If this block does not display your learning curve, please wait for few minutes, and re-run this block. It might take some time to load your logging information. 
"""

# Commented out IPython magic to ensure Python compatibility.
# %reload_ext tensorboard
# %tensorboard --logdir=./runs/

"""# Testing
The predictions of your model on testing set will be stored at `pred.csv`.
"""

def save_pred(preds, file):
    ''' Save predictions to specified file '''
    with open(file, 'w') as fp:
        writer = csv.writer(fp)
        writer.writerow(['id', 'tested_positive'])
        for i, p in enumerate(preds):
            writer.writerow([i, p])

model = My_Model(input_dim=x_train.shape[1]).to(device)
model.load_state_dict(torch.load(config['save_path']))
preds = predict(test_loader, model, device) 
save_pred(preds, 'pred.csv')

"""# Reference
This notebook uses code written by Heng-Jui Chang @ NTUEE (https://github.com/ga642381/ML2021-Spring/blob/main/HW01/HW01.ipynb)
"""
